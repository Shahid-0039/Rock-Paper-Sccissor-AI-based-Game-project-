# Rock Paper Scissors Game with Computer Vision

This project implements a Rock Paper Scissors game where players can use their webcam to show hand gestures (rock, paper, or scissors), which are detected using a pre-trained convolutional neural network (CNN) based on MobileNetV2. The game is played against the computer, which randomly selects its move. The project includes two main scripts: one for training the model (`RockPaperScissorstrain.py`) and one for running the game (`RockPaperScissors.py`).

## Features
- **Real-time gesture recognition**: Uses a webcam to detect hand gestures with a pre-trained MobileNetV2 model.
- **Interactive gameplay**: Play up to 3 rounds against the computer, with scores tracked and displayed.
- **Logging**: Saves game logs and captured hand gesture images for debugging and analysis.
- **Confidence-based detection**: Ensures reliable predictions with a confidence threshold of 70%.
- **User-friendly interface**: Displays game status, scores, and instructions on the webcam feed.

## Requirements
- Python 3.8+
- Libraries:
  - `tensorflow` (2.10 or later recommended)
  - `opencv-python` (for webcam capture and image processing)
  - `numpy`
- A webcam connected to your computer.
- A pre-trained model file (`best_model.keras`) generated by the training script.
- Dataset for training (organized into `train`, `val`, and `test` directories with subfolders for `rock`, `paper`, and `scissors`).

## Installation
1. **Clone the repository or download the scripts**:
   Ensure you have both `RockPaperScissors.py` and `RockPaperScissorstrain.py` in your project directory.

2. **Install dependencies**:
   Run the following command to install required Python libraries:
   ```bash
   pip install tensorflow opencv-python numpy
   ```

3. **Prepare the dataset** (for training):
   - Create directories named `train`, `val`, and `test`.
   - Inside each directory, create subdirectories named `rock`, `paper`, and `scissors`.
   - Populate these subdirectories with images of corresponding hand gestures.
   - Example directory structure:
     ```
     train/
       rock/
       paper/
       scissors/
     val/
       rock/
       paper/
       scissors/
     test/
       rock/
       paper/
       scissors/
     ```

4. **Train the model** (if not using a pre-trained model):
   - Run the training script to generate `best_model.keras`:
     ```bash
     python RockPaperScissorstrain.py
     ```
   - This script uses MobileNetV2 with fine-tuning, data augmentation, and callbacks for early stopping, learning rate reduction, and model checkpointing.
   - The trained model will be saved as `best_model.keras` and `rock_paper_scissors_model_final.h5`.

5. **Place the model file**:
   Ensure `best_model.keras` is in the same directory as `RockPaperScissors.py`.

## Usage
1. **Run the game**:
   Execute the game script:
   ```bash
   python RockPaperScissors.py
   ```

2. **Game instructions**:
   - A window will open showing your webcam feed with a green box in the center.
   - Place your hand inside the green box to show a rock, paper, or scissors gesture.
   - Press the `Space` key to start a round. A 3-second countdown will begin, followed by gesture detection.
   - The computer will randomly select its move, and the winner of the round will be displayed.
   - The game continues for 3 rounds. After the final round, the overall winner is shown.
   - Press `Space` to play again or `Q` to quit.

3. **Logs and debugging**:
   - Game logs are saved in the `logs` directory as `game_log.txt`.
   - Captured hand gesture images are saved in the `logs` directory with timestamps and confidence scores for debugging.

## Configuration
- **Model Path**: The game expects the model file at `best_model.keras`. Modify `MODEL_PATH` in `RockPaperScissors.py` if using a different file.
- **Confidence Threshold**: Set to 70% (`CONFIDENCE_THRESHOLD`). Adjust this in `RockPaperScissors.py` to make gesture detection more or less strict.
- **Maximum Rounds**: Set to 3 (`MAX_ROUNDS`). Change this in `RockPaperScissors.py` to adjust game length.
- **ROI Size**: The region of interest (ROI) for gesture detection is a 250x250 pixel box. Modify `roi_size` in `RockPaperScissors.py` if needed.

## Training Details
- **Model Architecture**: Uses MobileNetV2 with the top layers removed, followed by global average pooling, a dense layer with ReLU activation, dropout (0.5), and a softmax output layer.
- **Fine-Tuning**: The last 40 layers of MobileNetV2 are trainable (`FIN_TUNE_ATT` in `RockPaperScissorstrain.py`).
- **Data Augmentation**: Includes rotation, width/height shift, shear, zoom, brightness adjustment, and horizontal flip for robust training.
- **Callbacks**:
  - Early stopping based on validation accuracy.
  - Learning rate reduction on plateau.
  - Model checkpointing to save the best model based on validation accuracy.
- **Evaluation**: The script evaluates the model on the test set and prints test loss and accuracy.

## Notes
- Ensure your webcam is working and accessible by OpenCV.
- The model requires sufficient lighting and clear hand gestures for accurate predictions.
- If the model fails to load or the webcam cannot be accessed, check the logs in `logs/game_log.txt` for details.
- The training script assumes a dataset is available. You can use publicly available datasets like the [Rock-Paper-Scissors dataset on Kaggle](https://www.kaggle.com/datasets/drgfreeman/rockpaperscissors) or collect your own images.

## Troubleshooting
- **Model loading error**: Verify that `best_model.keras` exists and is not corrupted. Re-run `RockPaperScissorstrain.py` if needed.
- **Webcam issues**: Ensure no other applications are using the webcam. Check the device index in `cv2.VideoCapture(0)` and try other indices (e.g., 1, 2) if necessary.
- **Low prediction confidence**: Adjust `CONFIDENCE_THRESHOLD` or improve lighting and gesture clarity.
- **Training errors**: Ensure the dataset directories (`train`, `val`, `test`) are correctly structured and contain sufficient images.

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## Acknowledgments
- Built using [TensorFlow](https://www.tensorflow.org/) for model training and inference.
- Utilizes [OpenCV](https://opencv.org/) for webcam capture and image processing.
- Based on the MobileNetV2 architecture from [Keras Applications](https://keras.io/api/applications/).